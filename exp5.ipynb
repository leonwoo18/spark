{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   name|stuid|\n",
      "+-------+-----+\n",
      "|Michael|  329|\n",
      "|   Andy|  330|\n",
      "| 吴利洪|  134|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1=sc.parallelize([('Michael','329'),('Andy','330'),('吴利洪','134')])\n",
    "df=rdd1.toDF(['name','stuid'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(stuname='Michael', stuid='329'),\n",
       " Row(stuname='Andy', stuid='300'),\n",
       " Row(stuname='吴利洪', stuid='134')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "rdd=sc.parallelize([('Michael','329'),('Andy','300'),('吴利洪','134')])\n",
    "df=spark.createDataFrame(rdd,['stuname','stuid'])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- weight: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd1=sc.parallelize([('Michael',29,73.5)])\n",
    "schema = StructType([StructField('name',StringType(),False),\n",
    "                     StructField('age',IntegerType(),True),\n",
    "                     StructField('weight',FloatType(),True),])\n",
    "df = rdd1.toDF(schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize( [{'name': 'Alice', 'age': 25}])\n",
    "spark.createDataFrame(rdd, \"name:string,age:int\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'int')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize( [('Michael','329')])\n",
    "spark.createDataFrame(rdd, \"name: string, age: int\").dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(EMPNO=7369, ENAME='WuLihong', JOB='CLERK', MGR='7902', HIREDATE='1980/12/17', SAL=800, COMM=0, DEPTNO=20),\n",
       " Row(EMPNO=7499, ENAME='ALLEN', JOB='SALESMAN', MGR='7698', HIREDATE='1981/2/20', SAL=1600, COMM=300, DEPTNO=30),\n",
       " Row(EMPNO=7521, ENAME='WARD', JOB='SALESMAN', MGR='7698', HIREDATE='1981/2/22', SAL=1250, COMM=500, DEPTNO=30),\n",
       " Row(EMPNO=7566, ENAME='JONES', JOB='MANAGER', MGR='7839', HIREDATE='1981/4/2', SAL=2975, COMM=0, DEPTNO=20),\n",
       " Row(EMPNO=7654, ENAME='MARTIN', JOB='SALESMAN', MGR='7698', HIREDATE='1981/9/28', SAL=1250, COMM=1400, DEPTNO=30),\n",
       " Row(EMPNO=7698, ENAME='BLAKE', JOB='MANAGER', MGR='7839', HIREDATE='1981/5/1', SAL=2850, COMM=0, DEPTNO=30),\n",
       " Row(EMPNO=7782, ENAME='CLARK', JOB='MANAGER', MGR='7839', HIREDATE='1981/6/9', SAL=2450, COMM=0, DEPTNO=10),\n",
       " Row(EMPNO=7788, ENAME='SCOTT', JOB='ANALYST', MGR='7566', HIREDATE='1987/4/19', SAL=3000, COMM=0, DEPTNO=20),\n",
       " Row(EMPNO=7839, ENAME='KING', JOB='PRESIDENT', MGR='', HIREDATE='1981/11/17', SAL=5000, COMM=0, DEPTNO=10),\n",
       " Row(EMPNO=7844, ENAME='TURNER', JOB='SALESMAN', MGR='7698', HIREDATE='1981/9/8', SAL=1500, COMM=0, DEPTNO=30),\n",
       " Row(EMPNO=7876, ENAME='ADAMS', JOB='CLERK', MGR='7788', HIREDATE='1987/5/23', SAL=1100, COMM=0, DEPTNO=20),\n",
       " Row(EMPNO=7900, ENAME='JAMES', JOB='CLERK', MGR='7698', HIREDATE='1981/12/3', SAL=950, COMM=0, DEPTNO=30),\n",
       " Row(EMPNO=7902, ENAME='FORD', JOB='ANALYST', MGR='7566', HIREDATE='1981/12/3', SAL=3000, COMM=0, DEPTNO=20),\n",
       " Row(EMPNO=7934, ENAME='MILLER', JOB='CLERK', MGR='7782', HIREDATE='1982/1/23', SAL=1300, COMM=0, DEPTNO=10)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EMPNO: integer (nullable = true)\n",
      " |-- ENAME: string (nullable = true)\n",
      " |-- JOB: string (nullable = true)\n",
      " |-- MGR: string (nullable = true)\n",
      " |-- HIREDATE: string (nullable = true)\n",
      " |-- SAL: integer (nullable = true)\n",
      " |-- COMM: integer (nullable = true)\n",
      " |-- DEPTNO: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=sc.textFile(\"/exp05/emp.csv\",1)\n",
    "rdd1=rdd.map(lambda x:x.split(','))\n",
    "def turn(list):\n",
    "    r=[]\n",
    "    for x in list:\n",
    "        if x[6]=='':\n",
    "            x[6]='0'\n",
    "        r.append(x)\n",
    "    return r\n",
    "rdd2 = rdd1.mapPartitions(turn).map(lambda x:[int(x[0]),x[1],x[2],x[3],x[4],int(x[5]),int(x[6]),int(x[7])])                                   \n",
    "schema = StructType([StructField('EMPNO',IntegerType(),True),\n",
    "                  StructField(\"ENAME\",StringType(),True),\n",
    "                  StructField('JOB',StringType(),True),\n",
    "                  StructField('MGR',StringType(),True),\n",
    "                  StructField('HIREDATE',StringType(),True),\n",
    "                  StructField('SAL',IntegerType(),True),\n",
    "                  StructField('COMM',IntegerType(),True),\n",
    "                  StructField('DEPTNO',IntegerType(),True)])\n",
    "df=rdd2.toDF(schema)\n",
    "df.collect()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|deptno|      name|location|\n",
      "+------+----------+--------+\n",
      "|    10|ACCOUNTING|NEW YORK|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from spark.dept134 limit 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEPTNO=10, DNAME='ACCOUNTING', LOC='NEW YORK'),\n",
       " Row(DEPTNO=20, DNAME='RESEARCH', LOC='DALLAS'),\n",
       " Row(DEPTNO=30, DNAME='SALES', LOC='CHICAGO'),\n",
       " Row(DEPTNO=40, DNAME='OPERATIONS', LOC='BOSTON')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEPTNO: integer (nullable = true)\n",
      " |-- DNAME: string (nullable = true)\n",
      " |-- LOC: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd=sc.textFile(\"/exp05/dept.csv\")\n",
    "rdd1=rdd.map(lambda x:x.split(',')).map(lambda x:[int(x[0]),x[1],x[2]])\n",
    "df=rdd1.toDF(\"DEPTNO:int,DNAME:string,LOC:string\")\n",
    "df.collect()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+------+\n",
      "| id|      lat|       lon|  time|\n",
      "+---+---------+----------+------+\n",
      "|  5|30.546826|104.053415|111512|\n",
      "|  5|30.546835|104.052786|111522|\n",
      "|  5|30.546841|104.051586|111602|\n",
      "|  5|30.546848|104.054942|111454|\n",
      "|  5|30.546853|104.054094|111502|\n",
      "|  5|30.546855|104.052478|111552|\n",
      "|  5|30.546859|104.058034|111403|\n",
      "|  5| 30.54686|104.057516|111423|\n",
      "|  5|30.546862|104.052629|111542|\n",
      "|  5|30.546864|104.059257|111353|\n",
      "+---+---------+----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=sc.textFile(\"/exp05/taxi.csv\")\n",
    "rdd1=rdd.map(lambda line:line.split(','))\n",
    "df=rdd1.toDF(['id','lat','lon','time'])\n",
    "df.createOrReplaceTempView('taxi_view')\n",
    "spark.sql(\"select * from taxi_view where id='5'\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|amount|\n",
      "+------+\n",
      "|   384|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=sc.textFile(\"/exp05/taxi.csv\")\n",
    "rdd1=rdd.map(lambda line:line.split(','))\n",
    "df=rdd1.toDF(['id','lat','lon','time'])\n",
    "df.createOrReplaceTempView('taxi_view')\n",
    "spark.sql(\"select count(*) as amount from (select distinct id from taxi_view) \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|count(id)|\n",
      "+---------+\n",
      "|     2815|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=sc.textFile(\"/exp05/taxi.csv\")\n",
    "rdd1=rdd.map(lambda line:line.split(','))\n",
    "df=rdd1.toDF(['id','lat','lon','time'])\n",
    "df.createOrReplaceTempView('taxi_view')\n",
    "spark.sql(\"select count(id) from taxi_view where id='297'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------+------+---------+------------------+--------------------+\n",
      "|  seq|                             name|rating|    price|               pub|                 url|\n",
      "+-----+---------------------------------+------+---------+------------------+--------------------+\n",
      "| 5173| 動力取向精神醫學--臨床應用與實務|  10.0|   1200元|          心灵工坊|https://book.doub...|\n",
      "| 9929|                       水彩绘森活|  10.0|     29.8|    人民邮电出版社|https://book.doub...|\n",
      "|10124|殷周金文集成(修订增补本共8册)(精)|  10.0|2400.00元|          中华书局|https://book.doub...|\n",
      "|16628|                     纸雕游戏大书|  10.0|  99.00元|      重庆出版集团|https://book.doub...|\n",
      "|19103|                     Michelangelo|  10.0| $200.00 |           Taschen|https://book.doub...|\n",
      "|20063|                一支笔的快乐涂鸦2|  10.0|     29.8|    人民邮电出版社|https://book.doub...|\n",
      "|32781|                       亲亲宝贝装|  10.0|  28.00元|江西科学技术出版社|https://book.doub...|\n",
      "|32879|                   Photoshop7解像|  10.0|  68.00元|        海洋出版社|https://book.doub...|\n",
      "|45687|                 戚蓼生序本石头记|  10.0| 350.00元|    人民文学出版社|https://book.doub...|\n",
      "|52504|                    宇宙兄弟（7）|  10.0|   JPY580|            講談社|https://book.doub...|\n",
      "+-----+---------------------------------+------+---------+------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd = sc.textFile('/exp05/book.txt')\n",
    "rdd2 = rdd.map(lambda line:line.split(','))\n",
    "rdd3=rdd2.map(lambda arr:[arr[0],arr[1],float(arr[2]),arr[3],arr[4],arr[5]])\n",
    "schema = StructType([\n",
    "StructField(\"seq\", StringType(), True),\n",
    "StructField(\"name\", StringType(), True),\n",
    "StructField(\"rating\", FloatType(), True),\n",
    "StructField(\"price\", StringType(), True),\n",
    "StructField(\"pub\", StringType(), True),\n",
    "StructField(\"url\", StringType(),True)])\n",
    "df =rdd3.toDF(schema)\n",
    "df.createOrReplaceTempView('book134_view')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(name)|\n",
      "+-----------+\n",
      "|         23|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd = sc.textFile('/exp05/book.txt')\n",
    "rdd2 = rdd.map(lambda line:line.split(','))\n",
    "rdd3=rdd2.map(lambda arr:[arr[0],arr[1],float(arr[2]),arr[3],arr[4],arr[5]])\n",
    "schema = StructType([\n",
    "StructField(\"seq\", StringType(), True),\n",
    "StructField(\"name\", StringType(), True),\n",
    "StructField(\"rating\", FloatType(), True),\n",
    "StructField(\"price\", StringType(), True),\n",
    "StructField(\"pub\", StringType(), True),\n",
    "StructField(\"url\", StringType(),True)])\n",
    "df =rdd3.toDF(schema)\n",
    "df.createOrReplaceTempView('book134_view')\n",
    "spark.sql(\"select count(name) from book134_view where name LIKE '%微积分%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+------+\n",
      "|  seq|                       name|rating|\n",
      "+-----+---------------------------+------+\n",
      "|12224|                 艺术的故事|   9.5|\n",
      "|  341|               UNIX编程艺术|   9.5|\n",
      "|41142|        凡﹒高-世界艺术巨匠|   9.5|\n",
      "|  344|计算机程序设计艺术（第1卷）|   9.5|\n",
      "|52872|                 暴雪的艺术|   9.5|\n",
      "| 5506|    詹森艺术史（插图第7版）|   9.4|\n",
      "|31285|         世界艺术大师：德加|   9.4|\n",
      "|  175|计算机程序设计艺术（第2卷）|   9.4|\n",
      "|16664|           和孩子一起玩艺术|   9.4|\n",
      "|   83|计算机程序设计艺术（第1卷）|   9.3|\n",
      "+-----+---------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd = sc.textFile('/exp05/book.txt')\n",
    "rdd2 = rdd.map(lambda line:line.split(','))\n",
    "rdd3=rdd2.map(lambda arr:[arr[0],arr[1],float(arr[2]),arr[3],arr[4],arr[5]])\n",
    "schema = StructType([\n",
    "StructField(\"seq\", StringType(), True),\n",
    "StructField(\"name\", StringType(), True),\n",
    "StructField(\"rating\", FloatType(), True),\n",
    "StructField(\"price\", StringType(), True),\n",
    "StructField(\"pub\", StringType(), True),\n",
    "StructField(\"url\", StringType(),True)])\n",
    "df =rdd3.toDF(schema)\n",
    "df.createOrReplaceTempView('book134_view')\n",
    "spark.sql(\"select seq,name,rating from book134_view where name LIKE'%艺术%'\\\n",
    "          and rating>=9 and rating<=9.5 order by rating desc \").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+\n",
      "|                   pub|amount|\n",
      "+----------------------+------+\n",
      "|        人民文学出版社|  1437|\n",
      "|        上海译文出版社|  1426|\n",
      "|              中华书局|  1278|\n",
      "|            东立出版社|  1223|\n",
      "|生活·读书·新知三联书店|  1105|\n",
      "|        北京大学出版社|   948|\n",
      "|            译林出版社|   934|\n",
      "|            商务印书馆|   917|\n",
      "|        上海人民出版社|   829|\n",
      "|    广西师范大学出版社|   726|\n",
      "+----------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd = sc.textFile('/exp05/book.txt')\n",
    "rdd2 = rdd.map(lambda line:line.split(','))\n",
    "rdd3=rdd2.map(lambda arr:[arr[0],arr[1],float(arr[2]),arr[3],arr[4],arr[5]])\n",
    "schema = StructType([\n",
    "StructField(\"seq\", StringType(), True),\n",
    "StructField(\"name\", StringType(), True),\n",
    "StructField(\"rating\", FloatType(), True),\n",
    "StructField(\"price\", StringType(), True),\n",
    "StructField(\"pub\", StringType(), True),\n",
    "StructField(\"url\", StringType(),True)])\n",
    "df =rdd3.toDF(schema)\n",
    "df.createOrReplaceTempView('book134_view')\n",
    "spark.sql(\"select pub,count(*) as amount from book134_view group by pub order by amount desc \").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     AvgOfArtbook|\n",
      "+-----------------+\n",
      "|8.454914568835854|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd = sc.textFile('/exp05/book.txt')\n",
    "rdd2 = rdd.map(lambda line:line.split(','))\n",
    "rdd3=rdd2.map(lambda arr:[arr[0],arr[1],float(arr[2]),arr[3],arr[4],arr[5]])\n",
    "schema = StructType([\n",
    "StructField(\"seq\", StringType(), True),\n",
    "StructField(\"name\", StringType(), True),\n",
    "StructField(\"rating\", FloatType(), True),\n",
    "StructField(\"price\", StringType(), True),\n",
    "StructField(\"pub\", StringType(), True),\n",
    "StructField(\"url\", StringType(),True)])\n",
    "df =rdd3.toDF(schema)\n",
    "df.createOrReplaceTempView('book134_view')\n",
    "spark.sql(\"select avg(Rating) as AvgOfArtbook from book134_view where name LIKE'%艺术%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     AvgOfArtbook|\n",
      "+-----------------+\n",
      "|8.543648599012247|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd = sc.textFile('/exp05/book.txt')\n",
    "rdd2 = rdd.map(lambda line:line.split(','))\n",
    "rdd3=rdd2.map(lambda arr:[arr[0],arr[1],float(arr[2]),arr[3],arr[4],arr[5]])\n",
    "schema = StructType([\n",
    "StructField(\"seq\", StringType(), True),\n",
    "StructField(\"name\", StringType(), True),\n",
    "StructField(\"rating\", FloatType(), True),\n",
    "StructField(\"price\", StringType(), True),\n",
    "StructField(\"pub\", StringType(), True),\n",
    "StructField(\"url\", StringType(),True)])\n",
    "df =rdd3.toDF(schema)\n",
    "df.createOrReplaceTempView('book134_view')\n",
    "spark.sql(\"select avg(Rating) as AvgOfArtbook from book134_view \\\n",
    "where exists (select name,Rating from book134_view where name LIKE'%艺术%')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+\n",
      "|               pub|Rating|\n",
      "+------------------+------+\n",
      "|          心灵工坊|  10.0|\n",
      "|            講談社|  10.0|\n",
      "|          中华书局|  10.0|\n",
      "|      重庆出版集团|  10.0|\n",
      "|    人民邮电出版社|  10.0|\n",
      "|江西科学技术出版社|  10.0|\n",
      "|        海洋出版社|  10.0|\n",
      "|           Taschen|  10.0|\n",
      "|    人民文学出版社|  10.0|\n",
      "|        商务印书馆|   9.9|\n",
      "|    台湾尖端出版社|   9.9|\n",
      "|    山东文艺出版社|   9.9|\n",
      "|     HarperCollins|   9.9|\n",
      "|McGraw-HillMedical|   9.9|\n",
      "|            赤々舎|   9.9|\n",
      "|              青文|   9.9|\n",
      "|    机械工业出版社|   9.9|\n",
      "|       StMartinsPr|   9.9|\n",
      "|    上海古籍出版社|   9.9|\n",
      "|                \"\"|   9.9|\n",
      "+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd = sc.textFile('/exp05/book.txt')\n",
    "rdd2 = rdd.map(lambda line:line.split(','))\n",
    "rdd3=rdd2.map(lambda arr:[arr[0],arr[1],float(arr[2]),arr[3],arr[4],arr[5]])\n",
    "schema = StructType([\n",
    "StructField(\"seq\", StringType(), True),\n",
    "StructField(\"name\", StringType(), True),\n",
    "StructField(\"rating\", FloatType(), True),\n",
    "StructField(\"price\", StringType(), True),\n",
    "StructField(\"pub\", StringType(), True),\n",
    "StructField(\"url\", StringType(),True)])\n",
    "df =rdd3.toDF(schema)\n",
    "df.createOrReplaceTempView('book134_view')\n",
    "spark.sql(\"select pub,max(Rating) as Rating from book134_view group by pub order by Rating desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd = sc.textFile('/exp05/book.txt')\n",
    "rdd2 = rdd.map(lambda line:line.split(','))\n",
    "rdd3=rdd2.map(lambda arr:[arr[0],arr[1],float(arr[2]),arr[3],arr[4],arr[5]])\n",
    "schema = StructType([\n",
    "StructField(\"seq\", StringType(), True),\n",
    "StructField(\"name\", StringType(), True),\n",
    "StructField(\"rating\", FloatType(), True),\n",
    "StructField(\"price\", StringType(), True),\n",
    "StructField(\"pub\", StringType(), True),\n",
    "StructField(\"url\", StringType(),True)])\n",
    "df =rdd3.toDF(schema)\n",
    "\n",
    "df.createOrReplaceTempView('book134')\n",
    "df_134=spark.sql(\"select pub,max(Rating) as Rating from book134 group by pub order by Rating desc\")\n",
    "df_134.createOrReplaceTempView('Max')\n",
    "df_result=spark.sql(\"select  book134.seq,book134.name,Max.pub,Max.Rating from book134,Max \\\n",
    "                    Where book134.pub=Max.pub \\\n",
    "                    And book134.Rating=Max.Rating \\\n",
    "                    order by Rating desc\")\n",
    "df_result.write.format(\"csv\").save(\"/home/hadoop/best-134/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------+------------------------------------+------+\n",
      "|  seq|                                name|                                 pub|Rating|\n",
      "+-----+------------------------------------+------------------------------------+------+\n",
      "|  343|         计算机程序设计艺术（第3卷）|                      国防工业出版社|   9.8|\n",
      "|19040|                          巴洛克艺术|                  北京美术摄影出版社|   9.8|\n",
      "|  173|计算机程序设计艺术第2卷半数值算法...|                      清华大学出版社|   9.7|\n",
      "|16482|       我的第一本艺术游戏书（全2册）|                      光明日报出版社|   9.7|\n",
      "| 9918|                          幻想的艺术|                  上海人民美术出版社|   9.6|\n",
      "| 8282|                          艺术的故事|                      广西美术出版社|   9.6|\n",
      "|52872|                          暴雪的艺术|                    北京联合出版公司|   9.5|\n",
      "|31285|                  世界艺术大师：德加|                      河北美术出版社|   9.4|\n",
      "|19308|                        江南理景艺术|东南大学出版社（南京东南大学出版社）|   9.2|\n",
      "|12636|                          艺术家手册|          凤凰传媒集团江苏美术出版社|   9.1|\n",
      "+-----+------------------------------------+------------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('/home/hadoop/best-134')\n",
    "rdd=spark.createDataFrame(df.rdd)\n",
    "df=rdd.toDF('seq','name','pub','Rating')\n",
    "df.createOrReplaceTempView('book134_view')\n",
    "spark.sql('select * from book134_view where name like \"%艺术%\" order by rating desc').show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
