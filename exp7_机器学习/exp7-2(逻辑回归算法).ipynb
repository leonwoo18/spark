{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n",
      "+---+----------------+-----+--------------------+\n",
      "| id|            text|label|               words|\n",
      "+---+----------------+-----+--------------------+\n",
      "|  0| a b c d e spark|  1.0|[a, b, c, d, e, s...|\n",
      "|  1|             b d|  0.0|              [b, d]|\n",
      "|  2|     spark f g h|  1.0|    [spark, f, g, h]|\n",
      "|  3|hadoop mapreduce|  0.0| [hadoop, mapreduce]|\n",
      "+---+----------------+-----+--------------------+\n",
      "\n",
      "+---+----------------+-----+--------------------+--------------------+\n",
      "| id|            text|label|               words|            features|\n",
      "+---+----------------+-----+--------------------+--------------------+\n",
      "|  0| a b c d e spark|  1.0|[a, b, c, d, e, s...|(262144,[17222,27...|\n",
      "|  1|             b d|  0.0|              [b, d]|(262144,[27526,30...|\n",
      "|  2|     spark f g h|  1.0|    [spark, f, g, h]|(262144,[15554,24...|\n",
      "|  3|hadoop mapreduce|  0.0| [hadoop, mapreduce]|(262144,[42633,15...|\n",
      "+---+----------------+-----+--------------------+--------------------+\n",
      "\n",
      "17223:1.000000\n",
      "27527:1.000000\n",
      "28699:1.000000\n",
      "30914:1.000000\n",
      "227411:1.000000\n",
      "234658:1.000000\n",
      "+---+------------------+--------------------+----------+\n",
      "| id|              text|         probability|prediction|\n",
      "+---+------------------+--------------------+----------+\n",
      "|  4|       spark i j k|[0.15964077387874...|       1.0|\n",
      "|  5|             l m n|[0.83783256854767...|       0.0|\n",
      "|  6|spark hadoop spark|[0.06926633132976...|       1.0|\n",
      "|  7|     apache hadoop|[0.98215753334442...|       0.0|\n",
      "+---+------------------+--------------------+----------+\n",
      "\n",
      "(4, spark i j k) --> prob=[0.1596407738787475,0.8403592261212525], prediction=1.000000\n",
      "(5, l m n) --> prob=[0.8378325685476744,0.16216743145232562], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[0.06926633132976037,0.9307336686702395], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.9821575333444218,0.01784246665557808], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "#密集向量\n",
    "dv = Vectors.dense(2, 5, 8)\n",
    "#print(dv[1])\n",
    "\n",
    "#稀疏向量\n",
    "sv=Vectors.sparse(4, (1, 2, 3), (3, 5, 7))\n",
    "#print(sv[0]) \n",
    "\n",
    "from pyspark.ml import Pipeline,Transformer\n",
    "from pyspark.ml.feature import Tokenizer,HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "\t(3, \"hadoop mapreduce\", 0.0)], [\"id\", \"text\", \"label\"])\n",
    "training.show()\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "#通过Tokenizer:把text里的每个单词--->words[]数组里                                    ################\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")                            #              #\n",
    "                                                                                     #   Tokenizer  #\n",
    "words = tokenizer.transform(training)                                                #              # \n",
    "words.show()                                                                         ################\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "#通过HashTF:把word[]数组-->特征向量：                                        \n",
    "#(统计词频term frequency,由每个词所对应的唯一标识频率，来构成向量 )                  ################\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")      #               #\n",
    "                                                                                    #    HashTF     #\n",
    "df2=hashingTF.transform(words)                                                      #               #\n",
    "df2.show()                                                                          #################\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#打印id=o的text中所有词的频率\n",
    "v=df2.first()\n",
    "idx=0\n",
    "for val in v.features.toArray():\n",
    "    idx=idx+1\n",
    "    if val!=0:\n",
    "        print('%d:%f'%(idx,val))\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#逻辑回归算法，算法是一个Estimator                                                 #############\n",
    "# maxIter 最大迭代次数，regParam 是正则化参数，threshold是阈值                    #     Lr     #\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001,threshold=0.5)                ###############\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#Pipeline 连接 Transformer 和 Estimator                                 ##################################\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])                 #  Tokenizer \\                    #\n",
    "                                                                       #  HashTF    \\pipeline---> model  #\n",
    "#训练出模型，模型是 Transformer                                        #    Lr      \\                    # \n",
    "model = pipeline.fit(training)                                         ##################################\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#测试数据（不含标签）\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "#模型对测试数据进行预测，得出预测结果（DataFrame）\n",
    "prediction = model.transform(test)\n",
    "\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "selected.show()\n",
    "\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
